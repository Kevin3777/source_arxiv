2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Current SDK version is 0.15.12
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Configure stats pid to 1105
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Loading settings from /root/.config/wandb/settings
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Loading settings from /root/autodl-tmp/intrinsic-source-citation/wandb/settings
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-04-15 11:19:30,546 INFO    MainThread:1105 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program_abspath': '/root/autodl-tmp/intrinsic-source-citation/train.py', 'program': 'train.py'}
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:_log_setup():528] Logging user logs to /root/autodl-tmp/intrinsic-source-citation/wandb/run-20250415_111930-wt6ctlfa/logs/debug.log
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:_log_setup():529] Logging internal logs to /root/autodl-tmp/intrinsic-source-citation/wandb/run-20250415_111930-wt6ctlfa/logs/debug-internal.log
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:init():568] calling init triggers
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:init():575] wandb.init called with sweep_config: {}
config: {}
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:init():618] starting backend
2025-04-15 11:19:30,547 INFO    MainThread:1105 [wandb_init.py:init():622] setting up manager
2025-04-15 11:19:30,550 INFO    MainThread:1105 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-04-15 11:19:30,551 INFO    MainThread:1105 [wandb_init.py:init():628] backend started and connected
2025-04-15 11:19:30,558 INFO    MainThread:1105 [wandb_init.py:init():720] updated telemetry
2025-04-15 11:19:30,558 INFO    MainThread:1105 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2025-04-15 11:19:31,876 INFO    MainThread:1105 [wandb_run.py:_on_init():2220] communicating current version
2025-04-15 11:19:32,734 INFO    MainThread:1105 [wandb_run.py:_on_init():2229] got version response upgrade_message: "wandb version 0.19.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-04-15 11:19:32,734 INFO    MainThread:1105 [wandb_init.py:init():804] starting run threads in backend
2025-04-15 11:19:49,044 INFO    MainThread:1105 [wandb_run.py:_console_start():2199] atexit reg
2025-04-15 11:19:49,044 INFO    MainThread:1105 [wandb_run.py:_redirect():2054] redirect: wrap_raw
2025-04-15 11:19:49,044 INFO    MainThread:1105 [wandb_run.py:_redirect():2119] Wrapping output streams.
2025-04-15 11:19:49,044 INFO    MainThread:1105 [wandb_run.py:_redirect():2144] Redirects installed.
2025-04-15 11:19:49,045 INFO    MainThread:1105 [wandb_init.py:init():845] run started, returning control to user process
2025-04-15 11:19:49,061 INFO    MainThread:1105 [wandb_run.py:_config_callback():1324] config_cb None None {'num_nodes': 1, 'num_gpus_per_node': 1, 'node_name': 'unknown because NODENAME environment variable not set'}
2025-04-15 11:19:49,399 INFO    MainThread:1105 [wandb_run.py:_config_callback():1324] config_cb None None {'rank_zero_seed': 17}
2025-04-15 11:20:02,603 INFO    MainThread:1105 [wandb_run.py:_config_callback():1324] config_cb None None {'algorithms': {'gradient_clipping': {'clipping_threshold': 1.0, 'clipping_type': 'norm'}}, 'callbacks': {'lr_monitor': {}, 'memory_monitor': {}, 'runtime_estimator': {}, 'speed_monitor': {'window_size': 10}}, 'console_log_interval': '50ba', 'cross_doc_attention': False, 'dataloaders': [{'dataset': {'batch_type': 'lm', 'local': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/', 'max_seq_len': 2048, 'shuffle': True, 'split': 'train'}, 'drop_last': False, 'name': 'train_loader_docs', 'num_workers': 0}, {'dataset': {'max_seq_len': 2048, 'path': '/root/autodl-tmp/intrinsic-source-citation/dataset/ours/qa', 'split': 'qa_train'}, 'drop_last': False, 'name': 'in_domain_standard_q_answer_eval_loader', 'num_workers': 0}, {'dataset': {'max_seq_len': 2048, 'path': '/root/autodl-tmp/intrinsic-source-citation/dataset/ours/qa', 'split': 'qa_train'}, 'drop_last': False, 'name': 'out_of_domain_standard_q_answer_eval_loader', 'num_workers': 0}, {'dataset': {'batch_type': 'fact', 'local': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/qa', 'masking': {'cross_doc_attention': False}, 'max_seq_len': 2048, 'shuffle': True, 'split': 'qa_attribution_train'}, 'drop_last': False, 'name': 'train_q_a_url', 'num_workers': 0}], 'deepspeed_config': {'bf16': {'enabled': True}, 'train_batch_size': 80, 'zero_optimization': {'allgather_bucket_size': 200000000.0, 'contiguous_gradients': True, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'reduce_bucket_size': True, 'reduce_scatter': True, 'stage': 3}}, 'device_eval_batch_size': 40, 'device_train_microbatch_size': 2, 'eval_first': False, 'eval_interval': '1ep', 'eval_subset_num_batches': -1, 'experiment': {'data': {'augment': {'doc': {'do': False, 'method': 'permute', 'n_sample_per_doc': 2}}, 'finetune': {'neg_create_probability': 0.0, 'number_non_attributable_negatives': 0}, 'qa_data_path': '/root/autodl-tmp/intrinsic-source-citation/dataset/ours', 'text_data_path': 'dataset/ours/pretrain', 'train_data_path': '/root/autodl-tmp/intrinsic-source-citation/dataset/ours/pretrain/train'}, 'eval': {'disable_all_eval': False, 'disable_attribution_eval': False, 'disable_non_attrib_eval': True, 'disable_qa_eval': False, 'icl_eval': False, 'ppl_eval': False, 'use_ais': False}, 'experiment': {'name': 'arxiv-citation-doc-id-begin', 'output_dir': 'outputs/experiments/'}, 'model': {'name': 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'}, 'train': {'config_template_path': 'conf/templates/train_config.yaml', 'cross_doc_attention': False, 'device_eval_batch_size': 40, 'device_train_microbatch_size': 2, 'eval_first': False, 'finetune_q_a': False, 'finetune_q_a_doc_url': False, 'finetune_q_a_url': True, 'finetune_q_url_a': False, 'loss_type': 'mask', 'lr': 8e-05, 'max_duration': '10ep', 'pretrain': True, 'q_a_url_predict_url_only': False, 'repeat_url_across_doc': False, 'save_folder': None, 'sequential': False, 'url_location': 'first', 'url_loss_factor': 1.0, 'weight_decay': 0.02}}, 'global_seed': 17, 'global_train_batch_size': 80, 'log_to_console': True, 'loggers': {'wandb': {'project': 'intrinsic-source-citation'}}, 'max_duration': '10ep', 'max_seq_len': 2048, 'model': {'checkpoint': None, 'loss': {'type': 'mask', 'url_loss_factor': 1.0}, 'name': 'hf_causal_lm', 'pretrained': True, 'pretrained_model_name_or_path': 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'}, 'ood_url_trie': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/unseen_url_trie.pkl', 'optimizer': {'betas': [0.9, 0.98], 'eps': 1e-06, 'lr': 8e-05, 'name': 'deepspeed_adam', 'weight_decay': 0.02}, 'precision': 'amp_bf16', 'progress_bar': False, 'run_name': 'arxiv-citation-doc-id-begin', 'save_folder': None, 'save_interval': '1ep', 'save_num_checkpoints_to_keep': 1, 'scheduler': {'alpha_f': 0.1, 'name': 'linear_decay_with_warmup', 't_warmup': '1ep'}, 'seed': 17, 'streaming': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/', 'text_data_path': 'dataset/ours/pretrain', 'tokenizer': {'kwargs': {'model_max_length': 2048}, 'name': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming//tokenizer'}, 'tokenizer_name': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming//tokenizer', 'url_trie': 'outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/url_trie.pkl', 'dist_timeout': 600.0, 'n_gpus': 1, 'device_train_batch_size': 80, 'device_train_grad_accum': 40, 'n_params': 1100056576}
2025-04-15 11:20:02,604 INFO    MainThread:1105 [wandb_run.py:_config_callback():1324] config_cb None None {'enabled_algorithms/GradientClipping': True}
