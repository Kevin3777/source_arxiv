2025-04-02 09:16:40,228 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Current SDK version is 0.15.12
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Configure stats pid to 1217
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Loading settings from /root/.config/wandb/settings
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Loading settings from /root/autodl-tmp/intrinsic-source-citation/wandb/settings
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program_abspath': '/root/autodl-tmp/intrinsic-source-citation/train.py', 'program': 'train.py'}
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_setup.py:_flush():76] Applying login settings: {'api_key': '***REDACTED***'}
2025-04-02 09:16:40,229 INFO    MainThread:1217 [wandb_init.py:_log_setup():528] Logging user logs to /root/autodl-tmp/intrinsic-source-citation/wandb/run-20250402_091640-h0cx1dgv/logs/debug.log
2025-04-02 09:16:40,230 INFO    MainThread:1217 [wandb_init.py:_log_setup():529] Logging internal logs to /root/autodl-tmp/intrinsic-source-citation/wandb/run-20250402_091640-h0cx1dgv/logs/debug-internal.log
2025-04-02 09:16:40,230 INFO    MainThread:1217 [wandb_init.py:init():568] calling init triggers
2025-04-02 09:16:40,230 INFO    MainThread:1217 [wandb_init.py:init():575] wandb.init called with sweep_config: {}
config: {}
2025-04-02 09:16:40,230 INFO    MainThread:1217 [wandb_init.py:init():618] starting backend
2025-04-02 09:16:40,230 INFO    MainThread:1217 [wandb_init.py:init():622] setting up manager
2025-04-02 09:16:40,232 INFO    MainThread:1217 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-04-02 09:16:40,234 INFO    MainThread:1217 [wandb_init.py:init():628] backend started and connected
2025-04-02 09:16:40,240 INFO    MainThread:1217 [wandb_init.py:init():720] updated telemetry
2025-04-02 09:16:40,240 INFO    MainThread:1217 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2025-04-02 09:16:41,645 INFO    MainThread:1217 [wandb_run.py:_on_init():2220] communicating current version
2025-04-02 09:16:42,842 INFO    MainThread:1217 [wandb_run.py:_on_init():2229] got version response upgrade_message: "wandb version 0.19.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-04-02 09:16:42,842 INFO    MainThread:1217 [wandb_init.py:init():804] starting run threads in backend
2025-04-02 09:17:00,450 INFO    MainThread:1217 [wandb_run.py:_console_start():2199] atexit reg
2025-04-02 09:17:00,450 INFO    MainThread:1217 [wandb_run.py:_redirect():2054] redirect: wrap_raw
2025-04-02 09:17:00,450 INFO    MainThread:1217 [wandb_run.py:_redirect():2119] Wrapping output streams.
2025-04-02 09:17:00,450 INFO    MainThread:1217 [wandb_run.py:_redirect():2144] Redirects installed.
2025-04-02 09:17:00,451 INFO    MainThread:1217 [wandb_init.py:init():845] run started, returning control to user process
2025-04-02 09:17:00,467 INFO    MainThread:1217 [wandb_run.py:_config_callback():1324] config_cb None None {'num_nodes': 1, 'num_gpus_per_node': 1, 'node_name': 'unknown because NODENAME environment variable not set'}
2025-04-02 09:17:00,592 INFO    MainThread:1217 [wandb_run.py:_config_callback():1324] config_cb None None {'rank_zero_seed': 17}
2025-04-02 09:17:15,249 INFO    MainThread:1217 [wandb_run.py:_config_callback():1324] config_cb None None {'algorithms': {'gradient_clipping': {'clipping_threshold': 1.0, 'clipping_type': 'norm'}}, 'callbacks': {'lr_monitor': {}, 'memory_monitor': {}, 'runtime_estimator': {}, 'speed_monitor': {'window_size': 10}}, 'console_log_interval': '50ba', 'cross_doc_attention': False, 'dataloaders': [{'dataset': {'local': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming/', 'max_seq_len': 2048, 'split': 'train'}, 'drop_last': False, 'name': 'train_loader_docs', 'num_workers': 0}], 'deepspeed_config': {'bf16': {'enabled': True}, 'train_batch_size': 64, 'zero_optimization': {'allgather_bucket_size': 200000000.0, 'contiguous_gradients': True, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'overlap_comm': True, 'reduce_bucket_size': True, 'reduce_scatter': True, 'stage': 3}}, 'device_eval_batch_size': 32, 'device_train_microbatch_size': 2, 'eval_first': False, 'eval_interval': 1, 'eval_subset_num_batches': -1, 'experiment': {'data': {'augment': {'doc': {'do': True, 'method': 'permute', 'n_sample_per_doc': 2}}, 'finetune': {'neg_create_probability': 0.0, 'number_non_attributable_negatives': 0}, 'text_data_path': 'sample-data/biocite-1k/text'}, 'eval': {'disable_all_eval': True, 'disable_attribution_eval': False, 'disable_non_attrib_eval': True, 'disable_qa_eval': False, 'icl_eval': False, 'ppl_eval': True}, 'experiment': {'name': 'seq-training-doc-id-repeat-subset_pretrain', 'output_dir': 'outputs/experiments'}, 'model': {'name': 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'}, 'train': {'config_template_path': 'conf/templates/train_config.yaml', 'cross_doc_attention': False, 'device_eval_batch_size': 32, 'device_train_microbatch_size': 2, 'eval_first': False, 'finetune_q_a': False, 'finetune_q_a_doc_url': False, 'finetune_q_a_url': False, 'finetune_q_url_a': False, 'loss_type': 'mask', 'lr': 8e-05, 'pretrain': True, 'repeat_url_across_doc': True, 'sequential': True, 'url_location': 'last', 'url_loss_factor': 1.0, 'weight_decay': 0.02}}, 'global_seed': 17, 'global_train_batch_size': 64, 'log_to_console': True, 'loggers': {'wandb': {'project': 'intrinsic-source-citation'}}, 'max_duration': '10ep', 'max_seq_len': 2048, 'model': {'checkpoint': None, 'loss': {'type': 'mask', 'url_loss_factor': 1.0}, 'name': 'hf_causal_lm', 'pretrained': True, 'pretrained_model_name_or_path': 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'}, 'ood_url_trie': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming/unseen_url_trie.pkl', 'optimizer': {'betas': [0.9, 0.98], 'eps': 1e-06, 'lr': 8e-05, 'name': 'deepspeed_adam', 'weight_decay': 0.02}, 'precision': 'amp_bf16', 'progress_bar': False, 'run_name': 'seq-training-doc-id-repeat-subset_pretrain', 'save_folder': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/checkpoints', 'save_interval': '1ep', 'save_num_checkpoints_to_keep': 1, 'scheduler': {'alpha_f': 0.1, 'name': 'linear_decay_with_warmup', 't_warmup': '1ep'}, 'seed': 17, 'streaming': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming/', 'text_data_path': 'sample-data/biocite-1k/text', 'tokenizer': {'kwargs': {'model_max_length': 2048}, 'name': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming//tokenizer'}, 'tokenizer_name': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming//tokenizer', 'url_trie': 'outputs/experiments/seq-training-doc-id-repeat-subset_pretrain/data/streaming/url_trie.pkl', 'dist_timeout': 600.0, 'n_gpus': 1, 'device_train_batch_size': 64, 'device_train_grad_accum': 32, 'n_params': 1100056576}
2025-04-02 09:17:15,251 INFO    MainThread:1217 [wandb_run.py:_config_callback():1324] config_cb None None {'enabled_algorithms/GradientClipping': True}
2025-04-02 09:23:06,889 WARNING MsgRouterThr:1217 [router.py:message_loop():77] message_loop has been closed
