/root/miniconda3/lib/python3.8/site-packages/composer/callbacks/memory_monitor.py:86: UserWarning: The memory monitor only works on CUDA devices, but the model is on cpu.
  warnings.warn(f'The memory monitor only works on CUDA devices, but the model is on {model_device.type}.')
/root/miniconda3/lib/python3.8/site-packages/composer/callbacks/speed_monitor.py:120: UserWarning: gpu_flop count not found for None with precision: amp_bf16; MFU cannot be calculated and reported. gpu_flops_available can be manuallyoverridden by setting gpu_flops_available in SpeedMonitor.
  warnings.warn(
2025-04-14 20:09:58,325: rank0[11416][MainThread]: INFO: composer.trainer.trainer: Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
2025-04-14 20:09:58,326: rank0[11416][MainThread]: DEBUG: composer.trainer.trainer: Initializing deepspeed
[2025-04-14 20:09:58,326] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown
[2025-04-14 20:09:58,326] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-04-14 20:09:59,627] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-14 20:09:59,629] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-14 20:09:59,630] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-14 20:09:59,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-04-14 20:09:59,645] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-04-14 20:09:59,645] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-04-14 20:09:59,646] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-04-14 20:09:59,964] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning
[2025-04-14 20:09:59,966] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.15 GB         Max_CA 2 GB
[2025-04-14 20:09:59,966] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.72 GB, percent = 9.1%
[2025-04-14 20:09:59,968] [INFO] [stage3.py:127:__init__] Reduce bucket size 1
[2025-04-14 20:09:59,968] [INFO] [stage3.py:128:__init__] Prefetch bucket size 50,000,000
[2025-04-14 20:10:00,158] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-04-14 20:10:00,160] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.15 GB         Max_CA 2 GB
[2025-04-14 20:10:00,160] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.73 GB, percent = 9.1%
Parameter Offload: Total persistent parameters: 92160 in 45 params
[2025-04-14 20:10:00,405] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-04-14 20:10:00,406] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.18 GB         CA 2.4 GB         Max_CA 2 GB
[2025-04-14 20:10:00,406] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.73 GB, percent = 9.1%
[2025-04-14 20:10:00,567] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions
[2025-04-14 20:10:00,568] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.4 GB         Max_CA 2 GB
[2025-04-14 20:10:00,569] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.73 GB, percent = 9.1%
[2025-04-14 20:10:02,497] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 2
[2025-04-14 20:10:02,498] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.07 GB         Max_CA 2 GB
[2025-04-14 20:10:02,499] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.74 GB, percent = 9.1%
[2025-04-14 20:10:02,648] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions
[2025-04-14 20:10:02,649] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.07 GB         Max_CA 2 GB
[2025-04-14 20:10:02,649] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 91.74 GB, percent = 9.1%
[2025-04-14 20:10:04,804] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions
[2025-04-14 20:10:04,805] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.07 GB         Max_CA 2 GB
[2025-04-14 20:10:04,805] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 94.88 GB, percent = 9.4%
[2025-04-14 20:10:04,959] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2025-04-14 20:10:04,960] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.07 GB         Max_CA 2 GB
[2025-04-14 20:10:04,960] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 94.9 GB, percent = 9.4%
[2025-04-14 20:10:08,949] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2025-04-14 20:10:08,950] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.07 GB         Max_CA 2 GB
[2025-04-14 20:10:08,950] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 108.67 GB, percent = 10.8%
[2025-04-14 20:10:08,950] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
2025-04-14 20:10:11,011: rank0[11416][MainThread]: INFO: composer.trainer.trainer: Setting seed to 17
2025-04-14 20:10:11,011: rank0[11416][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17
2025-04-14 20:10:11,031: rank0[11416][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.AMP_BF16
******************************
Config:
enabled_algorithms/GradientClipping: true
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 17
******************************
2025-04-14 20:10:11,033: rank0[11416][MainThread]: DEBUG: composer.trainer.trainer: Spinning the dataloaders
[2025-04-14 20:10:10,999] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2025-04-14 20:10:11,000] [INFO] [utils.py:792:see_memory_usage] MA 2.06 GB         Max_MA 2.31 GB         CA 2.32 GB         Max_CA 2 GB
[2025-04-14 20:10:11,001] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 112.77 GB, percent = 11.2%
[2025-04-14 20:10:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2025-04-14 20:10:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-04-14 20:10:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-14 20:10:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.98]]
[2025-04-14 20:10:11,002] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-04-14 20:10:11,002] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   amp_params ................... False
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-04-14 20:10:11,003] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5ae039a520>
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   dump_state ................... False
[2025-04-14 20:10:11,004] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-04-14 20:10:11,005] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 40
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-04-14 20:10:11,006] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   optimizer_name ............... None
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   optimizer_params ............. None
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-14 20:10:11,007] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   pld_params ................... False
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   steps_per_print .............. 10
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   train_batch_size ............. 80
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-04-14 20:10:11,008] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   world_size ................... 1
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   zero_enabled ................. True
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-14 20:10:11,009] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3
[2025-04-14 20:10:11,009] [INFO] [config.py:974:print_user_config]   json = {
    "bf16": {
        "enabled": true
    },
    "train_batch_size": 80,
    "zero_optimization": {
        "allgather_bucket_size": 2.000000e+08,
        "contiguous_gradients": true,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "reduce_bucket_size": true,
        "reduce_scatter": true,
        "stage": 3
    },
    "gradient_clipping": 1.0,
    "gradient_accumulation_steps": 40,
    "train_micro_batch_size_per_gpu": 2,
    "zero_allow_untested_optimizer": true
}
Logging config...
algorithms:
  gradient_clipping:
    clipping_threshold: 1.0
    clipping_type: norm
callbacks:
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}
  speed_monitor:
    window_size: 10
console_log_interval: 50ba
cross_doc_attention: false
dataloaders:
- dataset:
    batch_type: lm
    local: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/
    max_seq_len: 2048
    shuffle: true
    split: train
  drop_last: false
  name: train_loader_docs
  num_workers: 0
- dataset:
    max_seq_len: 2048
    path: /root/autodl-tmp/intrinsic-source-citation/dataset/ours/qa
    split: qa_train
  drop_last: false
  name: in_domain_standard_q_answer_eval_loader
  num_workers: 0
- dataset:
    max_seq_len: 2048
    path: /root/autodl-tmp/intrinsic-source-citation/dataset/ours/qa
    split: qa_train
  drop_last: false
  name: out_of_domain_standard_q_answer_eval_loader
  num_workers: 0
- dataset:
    batch_type: fact
    local: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/qa
    masking:
      cross_doc_attention: false
    max_seq_len: 2048
    shuffle: true
    split: qa_attribution_train
  drop_last: false
  name: train_q_a_url
  num_workers: 0
deepspeed_config:
  bf16:
    enabled: true
  train_batch_size: 80
  zero_optimization:
    allgather_bucket_size: 200000000.0
    contiguous_gradients: true
    offload_optimizer:
      device: cpu
      pin_memory: true
    overlap_comm: true
    reduce_bucket_size: true
    reduce_scatter: true
    stage: 3
device_eval_batch_size: 40
device_train_microbatch_size: 2
eval_first: false
eval_interval: 1ep
eval_subset_num_batches: -1
experiment:
  data:
    augment:
      doc:
        do: false
        method: permute
        n_sample_per_doc: 2
    finetune:
      neg_create_probability: 0.0
      number_non_attributable_negatives: 0
    qa_data_path: /root/autodl-tmp/intrinsic-source-citation/dataset/ours
    text_data_path: dataset/ours/pretrain
    train_data_path: /root/autodl-tmp/intrinsic-source-citation/dataset/ours/pretrain/train
  eval:
    disable_all_eval: false
    disable_attribution_eval: false
    disable_non_attrib_eval: true
    disable_qa_eval: false
    icl_eval: false
    ppl_eval: false
    use_ais: false
  experiment:
    name: arxiv-citation-doc-id-begin
    output_dir: outputs/experiments/
  model:
    name: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
  train:
    config_template_path: conf/templates/train_config.yaml
    cross_doc_attention: false
    device_eval_batch_size: 40
    device_train_microbatch_size: 2
    eval_first: false
    finetune_q_a: false
    finetune_q_a_doc_url: false
    finetune_q_a_url: true
    finetune_q_url_a: false
    loss_type: mask
    lr: 8.0e-05
    max_duration: 10ep
    pretrain: true
    q_a_url_predict_url_only: false
    repeat_url_across_doc: false
    save_folder: null
    sequential: false
    url_location: first
    url_loss_factor: 1.0
    weight_decay: 0.02
global_seed: 17
global_train_batch_size: 80
log_to_console: true
loggers:
  wandb:
    project: intrinsic-source-citation
max_duration: 10ep
max_seq_len: 2048
model:
  checkpoint: null
  loss:
    type: mask
    url_loss_factor: 1.0
  name: hf_causal_lm
  pretrained: true
  pretrained_model_name_or_path: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
ood_url_trie: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/unseen_url_trie.pkl
optimizer:
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  lr: 8.0e-05
  name: deepspeed_adam
  weight_decay: 0.02
precision: amp_bf16
progress_bar: false
run_name: arxiv-citation-doc-id-begin
save_folder: null
save_interval: 1ep
save_num_checkpoints_to_keep: 1
scheduler:
  alpha_f: 0.1
  name: linear_decay_with_warmup
  t_warmup: 1ep
seed: 17
streaming: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/
text_data_path: dataset/ours/pretrain
tokenizer:
  kwargs:
    model_max_length: 2048
  name: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming//tokenizer
tokenizer_name: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming//tokenizer
url_trie: outputs/experiments/arxiv-citation-doc-id-begin/data/streaming/url_trie.pkl
dist_timeout: 600.0
n_gpus: 1
device_train_batch_size: 80
device_train_grad_accum: 40
n_params: 1100056576
Starting training...
[epoch=1][batch=1/134]:
	 Train time/epoch: 0
	 Train time/batch: 0
	 Train time/sample: 0
	 Train time/batch_in_epoch: 0
	 Train time/sample_in_epoch: 0
	 Train time/token: 0
	 Train time/token_in_epoch: 0
	 Train memory/allocated_mem: 3.4616
	 Train memory/active_mem: 3.4616
	 Train memory/inactive_mem: 0.1455
	 Train memory/reserved_mem: 20.4240
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 2.4038
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0000
	 Train time/train: 0.0093
	 Train time/val: 0.0000
	 Train time/total: 0.0093
[2025-04-14 20:15:49,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[4.7761194029850745e-06], mom=[[0.9, 0.98]]
[2025-04-14 20:15:49,314] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=10, RunningAvgSamplesPerSec=2.4843586250742837, CurrSamplesPerSec=2.4602023727588995, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 20:21:22,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.074626865671642e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:21:22,189] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=20, RunningAvgSamplesPerSec=2.5085652210126836, CurrSamplesPerSec=2.533623654042033, MemAllocated=3.83GB, MaxMemAllocated=17.59GB
[2025-04-14 20:26:50,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.671641791044776e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:26:50,778] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=30, RunningAvgSamplesPerSec=2.5253731997565394, CurrSamplesPerSec=2.5781144839740255, MemAllocated=3.84GB, MaxMemAllocated=17.59GB
[2025-04-14 20:32:22,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[2.2686567164179106e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:32:22,109] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=40, RunningAvgSamplesPerSec=2.528852336133087, CurrSamplesPerSec=2.574131880328727, MemAllocated=3.8GB, MaxMemAllocated=17.59GB
[2025-04-14 20:37:50,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[2.8656716417910447e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:37:50,413] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=50, RunningAvgSamplesPerSec=2.5351714599603636, CurrSamplesPerSec=2.607090107586341, MemAllocated=3.86GB, MaxMemAllocated=17.59GB
[epoch=1][batch=50/134]:
	 Train time/batch: 49
	 Train time/sample: 3920
	 Train time/batch_in_epoch: 49
	 Train time/sample_in_epoch: 3920
	 Train time/token: 3961920
	 Train time/token_in_epoch: 3961920
	 Train memory/allocated_mem: 3.4739
	 Train memory/active_mem: 3.4739
	 Train memory/inactive_mem: 0.1583
	 Train memory/reserved_mem: 27.6610
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 1.7855
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0000
	 Train time/remaining_estimate: 11.8782
	 Train time/train: 0.4609
	 Train time/val: 0.0000
	 Train time/total: 0.4609
	 Train throughput/batches_per_sec: 0.0305
	 Train throughput/samples_per_sec: 2.4368
	 Train throughput/device/batches_per_sec: 0.0305
	 Train throughput/device/samples_per_sec: 2.4368
[2025-04-14 20:43:28,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[3.462686567164179e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:43:28,464] [INFO] [timer.py:260:stop] epoch=0/micro_step=2400/global_step=60, RunningAvgSamplesPerSec=2.5264628344431235, CurrSamplesPerSec=2.4686819349031803, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 20:49:03,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[4.059701492537314e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:49:03,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=2800/global_step=70, RunningAvgSamplesPerSec=2.523284154771198, CurrSamplesPerSec=2.552059200196435, MemAllocated=3.82GB, MaxMemAllocated=17.59GB
[2025-04-14 20:54:40,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[4.6567164179104485e-05], mom=[[0.9, 0.98]]
[2025-04-14 20:54:40,351] [INFO] [timer.py:260:stop] epoch=0/micro_step=3200/global_step=80, RunningAvgSamplesPerSec=2.5196943676687753, CurrSamplesPerSec=2.45657459601604, MemAllocated=3.82GB, MaxMemAllocated=17.59GB
[2025-04-14 21:00:10,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[5.2537313432835826e-05], mom=[[0.9, 0.98]]
[2025-04-14 21:00:10,557] [INFO] [timer.py:260:stop] epoch=0/micro_step=3600/global_step=90, RunningAvgSamplesPerSec=2.522142027459946, CurrSamplesPerSec=2.499133071775073, MemAllocated=3.83GB, MaxMemAllocated=17.59GB
[2025-04-14 21:05:51,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[5.850746268656717e-05], mom=[[0.9, 0.98]]
[2025-04-14 21:05:51,861] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=100, RunningAvgSamplesPerSec=2.5161603643578982, CurrSamplesPerSec=2.4589756335756348, MemAllocated=3.86GB, MaxMemAllocated=17.59GB
[epoch=1][batch=100/134]:
	 Train time/batch: 99
	 Train time/sample: 7920
	 Train time/batch_in_epoch: 99
	 Train time/sample_in_epoch: 7920
	 Train time/token: 8008480
	 Train time/token_in_epoch: 8008480
	 Train memory/allocated_mem: 3.4721
	 Train memory/active_mem: 3.4721
	 Train memory/inactive_mem: 0.1434
	 Train memory/reserved_mem: 27.6610
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 1.6113
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0001
	 Train time/remaining_estimate: 11.5014
	 Train throughput/batches_per_sec: 0.0293
	 Train throughput/samples_per_sec: 2.3440
	 Train throughput/device/batches_per_sec: 0.0293
	 Train throughput/device/samples_per_sec: 2.3440
	 Train time/train: 0.9280
	 Train time/val: 0.0000
	 Train time/total: 0.9280
[2025-04-14 21:11:31,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[6.447761194029851e-05], mom=[[0.9, 0.98]]
[2025-04-14 21:11:31,054] [INFO] [timer.py:260:stop] epoch=0/micro_step=4400/global_step=110, RunningAvgSamplesPerSec=2.5137181123008987, CurrSamplesPerSec=2.536243249882753, MemAllocated=3.84GB, MaxMemAllocated=17.59GB
[2025-04-14 21:17:08,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[7.044776119402986e-05], mom=[[0.9, 0.98]]
[2025-04-14 21:17:08,563] [INFO] [timer.py:260:stop] epoch=0/micro_step=4800/global_step=120, RunningAvgSamplesPerSec=2.51235725479044, CurrSamplesPerSec=2.4563080155661483, MemAllocated=3.84GB, MaxMemAllocated=17.59GB
[2025-04-14 21:22:44,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[7.641791044776119e-05], mom=[[0.9, 0.98]]
[2025-04-14 21:22:44,041] [INFO] [timer.py:260:stop] epoch=0/micro_step=5200/global_step=130, RunningAvgSamplesPerSec=2.512319974721369, CurrSamplesPerSec=2.5575368059252197, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[Eval batch=1/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=13/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=26/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=38/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=51/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=63/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=75/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=88/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=100/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=113/125] Eval on in-domain-standard-q-answer-eval data
[Eval batch=125/125] Eval on in-domain-standard-q-answer-eval data:
	 Eval metrics/in-domain-standard-q-answer-eval/QA-EM: 0.0000
	 Eval metrics/in-domain-standard-q-answer-eval/QA-F1: 0.1529
	 Eval metrics/in-domain-standard-q-answer-eval/Hits@1-att: 0.0000
	 Eval metrics/in-domain-standard-q-answer-eval/Hits@10-att: 0.0000
[Eval batch=1/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=13/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=26/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=38/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=51/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=63/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=75/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=88/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=100/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=113/125] Eval on out-of-domain-standard-q-answer-eval data
[Eval batch=125/125] Eval on out-of-domain-standard-q-answer-eval data:
	 Eval metrics/out-of-domain-standard-q-answer-eval/QA-EM: 0.0000
	 Eval metrics/out-of-domain-standard-q-answer-eval/QA-F1: 0.1529
	 Eval metrics/out-of-domain-standard-q-answer-eval/Hits@1-att: 0.0000
	 Eval metrics/out-of-domain-standard-q-answer-eval/Hits@10-att: 0.0000
[2025-04-14 22:29:03,477] [WARNING] [stage3.py:1991:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2025-04-14 22:32:25,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[7.970149253731344e-05], mom=[[0.9, 0.98]]
[2025-04-14 22:32:25,031] [INFO] [timer.py:260:stop] epoch=0/micro_step=5600/global_step=140, RunningAvgSamplesPerSec=2.5129378349066966, CurrSamplesPerSec=2.4728953042199118, MemAllocated=3.86GB, MaxMemAllocated=17.59GB
[epoch=2][batch=16/134]:
	 Train time/epoch: 1
	 Train time/batch: 149
	 Train time/sample: 11883
	 Train time/batch_in_epoch: 15
	 Train time/sample_in_epoch: 1200
	 Train time/token: 12017962
	 Train time/token_in_epoch: 1219920
	 Train memory/allocated_mem: 3.4497
	 Train memory/active_mem: 3.4497
	 Train memory/inactive_mem: 10.7880
	 Train memory/reserved_mem: 24.7170
	 Train memory/alloc_retries: 2
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 1.1817
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0001
	 Train time/remaining_estimate: 20.7365
	 Train throughput/batches_per_sec: 0.0290
	 Train throughput/samples_per_sec: 2.3200
	 Train throughput/device/batches_per_sec: 0.0290
	 Train throughput/device/samples_per_sec: 2.3200
	 Train time/train: 1.3932
	 Train time/val: 1.0683
	 Train time/total: 2.4615
[2025-04-14 22:38:10,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[7.91044776119403e-05], mom=[[0.9, 0.98]]
[2025-04-14 22:38:10,069] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=150, RunningAvgSamplesPerSec=2.508020534961052, CurrSamplesPerSec=2.4952499653295304, MemAllocated=3.86GB, MaxMemAllocated=17.59GB
[2025-04-14 22:43:50,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[7.850746268656717e-05], mom=[[0.9, 0.98]]
[2025-04-14 22:43:50,172] [INFO] [timer.py:260:stop] epoch=0/micro_step=6400/global_step=160, RunningAvgSamplesPerSec=2.5060832851381836, CurrSamplesPerSec=2.613433236842753, MemAllocated=3.82GB, MaxMemAllocated=17.59GB
[2025-04-14 22:49:21,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[7.791044776119404e-05], mom=[[0.9, 0.98]]
[2025-04-14 22:49:21,403] [INFO] [timer.py:260:stop] epoch=0/micro_step=6800/global_step=170, RunningAvgSamplesPerSec=2.5080530718955916, CurrSamplesPerSec=2.506175337743186, MemAllocated=3.84GB, MaxMemAllocated=17.59GB
[2025-04-14 22:54:55,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[7.731343283582089e-05], mom=[[0.9, 0.98]]
[2025-04-14 22:54:55,338] [INFO] [timer.py:260:stop] epoch=0/micro_step=7200/global_step=180, RunningAvgSamplesPerSec=2.5086356535533154, CurrSamplesPerSec=2.6672618710683005, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 23:00:28,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[7.671641791044777e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:00:28,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=7600/global_step=190, RunningAvgSamplesPerSec=2.509320937149403, CurrSamplesPerSec=2.3975342741382306, MemAllocated=3.79GB, MaxMemAllocated=17.59GB
[epoch=2][batch=66/134]:
	 Train time/batch: 199
	 Train time/sample: 15883
	 Train time/batch_in_epoch: 65
	 Train time/sample_in_epoch: 5200
	 Train time/token: 16052682
	 Train time/token_in_epoch: 5254640
	 Train memory/allocated_mem: 3.4184
	 Train memory/active_mem: 3.4184
	 Train memory/inactive_mem: 6.1027
	 Train memory/reserved_mem: 24.7170
	 Train memory/alloc_retries: 2
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 1.0323
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0001
	 Train time/remaining_estimate: 20.2743
	 Train throughput/batches_per_sec: 0.0299
	 Train throughput/samples_per_sec: 2.3896
	 Train throughput/device/batches_per_sec: 0.0299
	 Train throughput/device/samples_per_sec: 2.3896
	 Train time/train: 1.8579
	 Train time/val: 1.0683
	 Train time/total: 2.9261
[2025-04-14 23:06:01,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[7.611940298507464e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:06:01,391] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=200, RunningAvgSamplesPerSec=2.510424791369347, CurrSamplesPerSec=2.7240775380981948, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 23:11:29,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[7.55223880597015e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:11:29,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=8400/global_step=210, RunningAvgSamplesPerSec=2.5127335556811996, CurrSamplesPerSec=2.4767235163375987, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 23:17:09,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[7.492537313432836e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:17:09,422] [INFO] [timer.py:260:stop] epoch=0/micro_step=8800/global_step=220, RunningAvgSamplesPerSec=2.5107993653122938, CurrSamplesPerSec=2.4650136790923654, MemAllocated=3.86GB, MaxMemAllocated=17.59GB
[2025-04-14 23:22:46,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[7.432835820895523e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:22:46,116] [INFO] [timer.py:260:stop] epoch=0/micro_step=9200/global_step=230, RunningAvgSamplesPerSec=2.5101510635179403, CurrSamplesPerSec=2.4046224110617915, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
[2025-04-14 23:28:36,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[7.373134328358209e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:28:36,370] [INFO] [timer.py:260:stop] epoch=0/micro_step=9600/global_step=240, RunningAvgSamplesPerSec=2.5055060287347146, CurrSamplesPerSec=2.2674304120550657, MemAllocated=3.83GB, MaxMemAllocated=17.59GB
[epoch=2][batch=116/134]:
	 Train time/batch: 249
	 Train time/sample: 19883
	 Train time/batch_in_epoch: 115
	 Train time/sample_in_epoch: 9200
	 Train time/token: 20100842
	 Train time/token_in_epoch: 9302800
	 Train memory/allocated_mem: 3.4740
	 Train memory/active_mem: 3.4740
	 Train memory/inactive_mem: 6.0534
	 Train memory/reserved_mem: 24.7170
	 Train memory/alloc_retries: 2
	 Train trainer/device_train_microbatch_size: 2
	 Train loss/train/total: 0.8217
	 Train lr-DeepSpeedZeroOptimizer_Stage3/group0: 0.0001
	 Train time/remaining_estimate: 19.8547
	 Train throughput/batches_per_sec: 0.0280
	 Train throughput/samples_per_sec: 2.2402
	 Train throughput/device/batches_per_sec: 0.0280
	 Train throughput/device/samples_per_sec: 2.2402
	 Train time/train: 2.3325
	 Train time/val: 1.0683
	 Train time/total: 3.4007
[2025-04-14 23:34:32,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[7.313432835820896e-05], mom=[[0.9, 0.98]]
[2025-04-14 23:34:32,171] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=250, RunningAvgSamplesPerSec=2.4999096939636227, CurrSamplesPerSec=2.4019075283949576, MemAllocated=3.85GB, MaxMemAllocated=17.59GB
2025-04-14 23:36:20,001: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing the engine
2025-04-14 23:36:20,002: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback WandBLogger
2025-04-14 23:36:20,002: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback ConsoleLogger
2025-04-14 23:36:20,003: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback LRMonitor
2025-04-14 23:36:20,003: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback MemoryMonitor
2025-04-14 23:36:20,003: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback RuntimeEstimator
2025-04-14 23:36:20,003: rank0[11416][MainThread]: DEBUG: composer.core.engine: Closing callback SpeedMonitor
2025-04-14 23:36:20,003: rank0[11416][MainThread]: DEBUG: composer.core.engine: Post-closing callback WandBLogger